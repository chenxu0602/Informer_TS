{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads \n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        assert self.head_dim * self.num_heads == self.d_model, \"Embedding size needs to be divisible by num_heads!\"\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "\n",
    "        # self.qkv_linear = nn.Linear(d_model, d_model * 3)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # N = query.shape[0]\n",
    "        # QKV = self.qkv_linear(torch.cat([query, key, value], dim=0))\n",
    "        # Q, K, V = torch.split(QKV, [N, N, N], dim=0)\n",
    "\n",
    "        # Q = Q.reshape(N, -1, self.num_heads, self.head_dim)\n",
    "        # K = K.reshape(N, -1, self.num_heads, self.head_dim)\n",
    "        # V = V.reshape(N, -1, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        Q = Q.view(Q.shape[0], -1, self.num_heads, self.head_dim)\n",
    "        K = K.view(K.shape[0], -1, self.num_heads, self.head_dim)\n",
    "        V = V.view(V.shape[0], -1, self.num_heads, self.head_dim)\n",
    "\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K]) / (self.head_dim ** 0.5)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, V]).reshape(Q.shape[0], -1, self.d_model)\n",
    "\n",
    "        return self.fc_out(out), attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbSparseMultiHeadAttention(MultiHeadAttention):\n",
    "    def __init__(self, d_model, num_heads, attention_ratio=0.25):\n",
    "        super().__init__(d_model, num_heads)\n",
    "        self.attention_ratio = attention_ratio\n",
    "\n",
    "    def forward(self,query, key, value, mask):\n",
    "        # N = query.shape[0]\n",
    "        # QKV = self.qkv_linear(torch.cat([query, key, value], dim=0))\n",
    "        # Q, K, V = torch.split(QKV, [N, N, N], dim=0)\n",
    "\n",
    "        # Q = Q.reshape(N, -1, self.num_heads, self.head_dim)\n",
    "        # K = K.reshape(N, -1, self.num_heads, self.head_dim)\n",
    "        # V = V.reshape(N, -1, self.num_heads, self.head_dim)\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        Q = Q.view(Q.shape[0], -1, self.num_heads, self.head_dim)\n",
    "        K = K.view(K.shape[0], -1, self.num_heads, self.head_dim)\n",
    "        V = V.view(V.shape[0], -1, self.num_heads, self.head_dim)\n",
    "\n",
    "        scaled_attention_logits = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K]) / (self.head_dim ** 0.5)\n",
    "        top_val, top_idx = torch.topk(scaled_attention_logits,\n",
    "                                      int(self.attention_ratio * scaled_attention_logits.shape[-1]), \n",
    "                                      sorted=False)\n",
    "        attention = torch.zeros_like(scaled_attention_logits).scatter_(-1, top_idx, top_val)\n",
    "\n",
    "        # seq_Q, seq_K = attention.size(-2), attention.size(-1)\n",
    "        # mask = torch.triu(torch.ones(seq_Q, seq_K), diagonal=1)\n",
    "        attention += (mask * -1e9)\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "        out = torch.einsum(\"bhql,blhd->bqhd\", [attention, V]).reshape(batch_size, -1, self.d_model)\n",
    "        return self.fc_out(out), attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ProbSparseMultiHeadAttention.forward() missing 1 required positional argument: 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m key   \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(batch_size, seq_len, d_model)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(batch_size, seq_len, d_model)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m output, attention_weights \u001b[39m=\u001b[39m mha(query, key, value)\n\u001b[1;32m     13\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mprint(f'Output shape: {output.shape}')\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint(f'Attneion weight shape: {attention_weights.shape}')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mprint(f'softmax attention shape: {attention.shape}')\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/GPT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: ProbSparseMultiHeadAttention.forward() missing 1 required positional argument: 'mask'"
     ]
    }
   ],
   "source": [
    "mha = ProbSparseMultiHeadAttention(512, 8).to(device)\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 100\n",
    "d_model = 512\n",
    "\n",
    "query = torch.rand(batch_size, seq_len, d_model).to(device)\n",
    "key   = torch.rand(batch_size, seq_len, d_model).to(device)\n",
    "value = torch.rand(batch_size, seq_len, d_model).to(device)\n",
    "\n",
    "output, attention_weights = mha(query, key, value)\n",
    "\n",
    "\"\"\"\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Attneion weight shape: {attention_weights.shape}')\n",
    "\n",
    "q_linear = nn.Linear(d_model, d_model)\n",
    "k_linear = nn.Linear(d_model, d_model)\n",
    "v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "qkv_linear = nn.Linear(d_model, d_model * 3)\n",
    "\n",
    "Q = q_linear(query)\n",
    "K = k_linear(key)\n",
    "V = v_linear(value)\n",
    "\n",
    "N = query.shape[0]\n",
    "\n",
    "QKV = qkv_linear(query)\n",
    "Q2, K2, V2 = torch.split(QKV, split_size_or_sections=d_model, dim=-1)\n",
    "\n",
    "n_heads = 8\n",
    "head_dim = d_model // n_heads\n",
    "\n",
    "Q = Q.view(Q.shape[0], -1, n_heads, head_dim)\n",
    "K = K.view(K.shape[0], -1, n_heads, head_dim)\n",
    "V = V.view(V.shape[0], -1, n_heads, head_dim)\n",
    "\n",
    "print(f'Q shape: {Q.shape}')\n",
    "print(f'K shape: {K.shape}')\n",
    "print(f'V shape: {V.shape}')\n",
    "\n",
    "scaled_attention_logits = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K]) / (head_dim ** 0.5)\n",
    "print(f\"scaled_attention_logits shape: {scaled_attention_logits.shape}\")\n",
    "\n",
    "top_val, top_idx = torch.topk(scaled_attention_logits, int(0.25 * scaled_attention_logits.shape[-1]), sorted=False)\n",
    "print(f'top_val shape: {top_val.shape}')\n",
    "print(f'top_idx shape: {top_idx.shape}')\n",
    "\n",
    "attention = torch.zeros_like(scaled_attention_logits).scatter_(-1, top_idx, top_val)\n",
    "print(f'attention shape: {attention.shape}')\n",
    "\n",
    "attention = F.softmax(attention, dim=-1)\n",
    "print(f'softmax attention shape: {attention.shape}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe.shape: torch.Size([5000, 512])\n",
      "position.shape: torch.Size([5000, 1])\n",
      "div_term.shape: torch.Size([256])\n",
      "pe.shape: torch.Size([5000, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "max_len = 5000\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.)) / d_model))\n",
    "print(f'pe.shape: {pe.shape}')\n",
    "print(f'position.shape: {position.shape}')\n",
    "print(f'div_term.shape: {div_term.shape}')\n",
    "\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "print(f'pe.shape: {pe.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "positionwise_feedforward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "x = torch.rand(32, 100, d_model)\n",
    "\n",
    "output = positionwise_feedforward(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # self.feed_forward = nn.Sequential(\n",
    "        #     nn.Linear(d_model, dim_feedforward),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(dim_feedforward, d_model)\n",
    "        # )\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.layernorm1(x)\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.layernorm2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "class InformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            InformerEncoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "num_layers = 3\n",
    "\n",
    "encoder_layer = InformerEncoderLayer(512, 8, 2048, 0.1)\n",
    "informer_encoder = InformerEncoder(d_model=512, num_heads=8, dim_feedforward=2048, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "src = torch.rand(32, 100, 512)\n",
    "\n",
    "mask = torch.triu(torch.ones(100, 100), diagonal=1).to(device)\n",
    "output = informer_encoder(src, mask)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.self_attn = ProbSparseMultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.layernorm1(x)\n",
    "\n",
    "        attn_output, _ = self.cross_attn(x, memory, memory)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.layernorm2(x)\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.layernorm3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m32\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m512\u001b[39m)\n\u001b[1;32m     23\u001b[0m memory \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m32\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m512\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m output \u001b[39m=\u001b[39m informer_decoder(x, memory)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/GPT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[67], line 15\u001b[0m, in \u001b[0;36mInformerDecoder.forward\u001b[0;34m(self, x, memory)\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(x)\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_layers:\n\u001b[0;32m---> 15\u001b[0m     x \u001b[39m=\u001b[39m layer(x, memory)\n\u001b[1;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/GPT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[66], line 16\u001b[0m, in \u001b[0;36mInformerDecoderLayer.forward\u001b[0;34m(self, x, memory)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, memory):\n\u001b[0;32m---> 16\u001b[0m     attn_output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x, mask)\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attn_output)\n\u001b[1;32m     18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm1(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/GPT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[50], line 33\u001b[0m, in \u001b[0;36mProbSparseMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(scaled_attention_logits)\u001b[39m.\u001b[39mscatter_(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, top_idx, top_val)\n\u001b[1;32m     31\u001b[0m \u001b[39m# seq_Q, seq_K = attention.size(-2), attention.size(-1)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# mask = torch.triu(torch.ones(seq_Q, seq_K), diagonal=1)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m attention \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m (mask \u001b[39m*\u001b[39;49m \u001b[39m-\u001b[39;49m\u001b[39m1e9\u001b[39;49m)\n\u001b[1;32m     35\u001b[0m attention \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(attention, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbhql,blhd->bqhd\u001b[39m\u001b[39m\"\u001b[39m, [attention, V])\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "class InformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            InformerDecoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, memory)\n",
    "\n",
    "        return x\n",
    "\n",
    "num_layers = 3\n",
    "informer_decoder = InformerDecoder(d_model=512, num_heads=8, dim_feedforward=2048, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "x = torch.rand(32, 100, 512)\n",
    "memory = torch.rand(32, 100, 512)\n",
    "\n",
    "output = informer_decoder(x, memory)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "InformerEncoder.forward() missing 1 required positional argument: 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m32\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m512\u001b[39m)\n\u001b[1;32m     18\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m32\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m512\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m output \u001b[39m=\u001b[39m informer_model(x, y)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/GPT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[60], line 10\u001b[0m, in \u001b[0;36mInformer.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[0;32m---> 10\u001b[0m     memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     11\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(y, memory)\n\u001b[1;32m     12\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/GPT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: InformerEncoder.forward() missing 1 required positional argument: 'mask'"
     ]
    }
   ],
   "source": [
    "class Informer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, num_encoder_layers, num_decoder_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = InformerEncoder(d_model, num_heads, dim_feedforward, num_encoder_layers, dropout)\n",
    "        self.decoder = InformerDecoder(d_model, num_heads, dim_feedforward, num_decoder_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        memory = self.encoder(x)\n",
    "        output = self.decoder(y, memory)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n",
    "\n",
    "informer_model = Informer(d_model=512, num_heads=8, dim_feedforward=2048, num_encoder_layers=3, num_decoder_layers=3, dropout=0.1)\n",
    "\n",
    "x = torch.rand(32, 100, 512)\n",
    "y = torch.rand(32, 100, 512)\n",
    "\n",
    "output = informer_model(x, y)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "data_root = Path(\"~/Work/Crypto/spot/monthly/klines\")\n",
    "yearmon = \"2023-07\"\n",
    "\n",
    "def load_raw_data(symbol):\n",
    "    filename = data_root / symbol / '1m' / f'{symbol}-1m-{yearmon}.csv'\n",
    "    df = pd.read_csv(filename,\n",
    "                     header=None,\n",
    "                     names=[\n",
    "                        \"open_time\",\n",
    "                        \"open\",\n",
    "                        \"high\",\n",
    "                        \"low\",\n",
    "                        \"close\",\n",
    "                        \"volume\",\n",
    "                        \"close_time\",\n",
    "                        \"quote_volume\",\n",
    "                        \"n_trades\",\n",
    "                        \"taker_buy_base_volume\",\n",
    "                        \"taker_buy_quote_volume\",\n",
    "                        \"ignore\",\n",
    "                     ])\n",
    "\n",
    "    df.set_index(\"open_time\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "df1 = load_raw_data(\"BTCUSDT\")\n",
    "df2 = load_raw_data(\"ETHUSDT\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"BTC\": np.log(df1[\"open\"]).diff(),\n",
    "    \"ETH\": np.log(df2[\"open\"]).diff(),\n",
    "}).dropna(how=\"any\")\n",
    "\n",
    "tensor_data: torch.Tensor = torch.FloatTensor(df.values)\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: torch.Tensor, src_len: int, tgt_len: int) -> None:\n",
    "        self.data = data\n",
    "        self.src_len = src_len\n",
    "        self.tgt_len = tgt_len \n",
    "        self.total_len = src_len + tgt_len \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.total_len + 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        start = idx \n",
    "        end = idx + self.total_len\n",
    "        sample = self.data[start:end]\n",
    "        src = sample[:self.src_len]\n",
    "        tgt = sample[self.src_len:]\n",
    "        return src, tgt\n",
    "\n",
    "src_seq_len: int = 60 * 48\n",
    "tgt_seq_len: int = 60\n",
    "batch_szie:  int = 32\n",
    "\n",
    "custom_dataset = TimeSeriesDataset(tensor_data, src_seq_len, tgt_seq_len)\n",
    "train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "InformerEncoder.forward() missing 1 required positional argument: 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:10\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/GPT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[60], line 10\u001b[0m, in \u001b[0;36mInformer.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[0;32m---> 10\u001b[0m     memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     11\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(y, memory)\n\u001b[1;32m     12\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/GPT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: InformerEncoder.forward() missing 1 required positional argument: 'mask'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = Informer(d_model=2, num_heads=2, dim_feedforward=2048, num_encoder_layers=3, num_decoder_layers=3, dropout=0.1)\n",
    "model.to(\"mps\")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=1e-4)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(\"mps\"), y.to(\"mps\")\n",
    "        outputs = model(x, y)\n",
    "\n",
    "        loss = criterion(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2880, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
